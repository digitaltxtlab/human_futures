{
    "collab_server" : "",
    "contents" : "# preprocessing\nrm(list = ls())\n# wd <- 'C:/Users/KLN/some_r'\n#wd = '~/courses/au_summer_university/summer_u2016/classes/tutorials'\nwd <- '/home/kln/Documents/education/tm_R/some_r';\nsetwd(wd)\ngetwd()\n\nsource(\"util_fun.R\")# input and parse file\n\n### data extraction\n# make data directory\ndd <-paste(wd,'/data',sep=\"\")\ndir.create(dd)\n\n\n# download file\nfilename.v = paste(dd,'/kjv.txt',sep=\"\")\nif (!file.exists(filename.v)) { download.file('http://www.gutenberg.org/cache/epub/10/pg10.txt', destfile = filename.v) }\n# import text lines\ntext.v <- scan(filename.v, what = 'character', sep='\\n', encoding = 'UTF-8')\n#print(text.v)\n#head(text.v)\n#tail(text.v)\n\n\n# separate data from metadata\nstart.v <- which(text.v == '*** START OF THIS PROJECT GUTENBERG EBOOK THE KING JAMES BIBLE ***')\nend.v <- which(text.v == '*** END OF THIS PROJECT GUTENBERG EBOOK THE KING JAMES BIBLE ***')\nmetadata.v <- text.v[c(1:start.v,end.v:length(text.v))]# create metadata variable\ntextlines.v <- text.v[(start.v+2):(end.v-2)]# remove metadata and title/end\nhead(textlines.v)\ntail(textlines.v)\nprint(length(text.v))\n#collapse lines\ntext.v <- paste(textlines.v, collapse = \" \")\nlength(text.v)\nhead(text.v)\n### preprocesing\n\n## casefolding\ntext.v <- tolower(text.v)\n\n## tokenization\ntokens.l <- strsplit(text.v, '\\\\W')# split on all non-alphanumeric characters with regex meta character\nclass(tokens.l)\nstr(tokens.l)\ntokens.l[[1]][1:20]\n# names(tokens.l) <- 'token'\n# tokens.l$token[100]\n# alternate for other character systems\n# tokensalt.l <- strsplit(text.v, '\\\\s')# split on whitespace characters\n\n\n# the love-hate ratio of KJV\nlhr <- sum(tokens.l[[1]] == 'love')/sum(tokens.l[[1]] == 'hate')\n\n# transform list to vector\ntokens.v <- unlist(tokens.l)\ntokens.v[1:25]# notice blanks and numerals\n\n## filtering\n# remove blanks (or any other token)\nidx <- which(tokens.v!=\"\")\ntokens.v <- tokens.v[idx]\ntokens.v[1:25]\nn1 <- length(tokens.v)\n\n# remove numerals\nidx <- grep(\"\\\\D\",tokens.v)# Not a digit [^0-9]\ntokens.fil.v <- tokens.v[idx]\ntokens.fil.v[1:25]\n\n\n\n# remove stop words\nlength(tokens.fil.v)\nstopword.v = unlist(lapply(read.csv(\"stoplist.csv\"),as.character),use.names=FALSE)\nhead(stopword.v)\nfor (w in 1:length(stopword.v)){\n  idx <- which(tokens.fil.v!=stopword.v[w])\n  tokens.fil.v <- tokens.fil.v[idx]\n}\nlength(tokens.fil.v)\ntokens.fil.v[1:25]\n\n# stem words with porter stemmer\ninstall.packages('SnowballC')\nlibrary(SnowballC)\ngetStemLanguages()\ntokens.fil.v <- wordStem(tokens.fil.v,'english')\ntokens.fil.v[1:25]\n\n# data reduction from preprocessing\nn2 <- length(tokens.fil.v)\nprint(n2/n1)\n\n# update the love-hate ratio\nlhr[2] <- sum(tokens.fil.v == 'love')/sum(tokens.fil.v == 'hate')\ndev.new()\npar(mfrow = c(2,1))\nbarplot(c(n1,n2), main = 'data reduction', names.arg=c('Before','After'))\nbarplot(lhr, main = 'Love-Hate Ratio in KJV', names.arg=c('Before','After'))\n\n# save tokenized and filtered data\nsave(filename.v,tokens.v,tokens.fil.v,file = 'kjv.RData')\n\n## synonym substitution based on WordNet database\nload('kjv.RData')\nlibrary(\"wordnet\")\n# for windows: setDict(\"C:/Program Files (x86)/WordNet/2.1/dict\")# set dictionary\nterm <- 'joy'\nsyn.v <- synonyms(term,'NOUN')\n# function for substituting synonyms\nsyno_replace <- function(term,tokens){\n  synos <- synonyms(term,'NOUN')\n  for (i in 1:length(synos)){\n    idx <- which(tokens == synos[i])\n    tokens[idx] <- term\n  }\n  return(tokens)\n}\nlength(which(tokens.v == term))\ntokens.v <- syno_replace(term,tokens.v)\nlength(which(tokens.v == term))\n\n# similar for strings\nstr <- 'a man is prepared for warfare and going to war'\ngsub('warfare','war',str)\nsyno_str_replace <- function(term,str){\n  synos <- synonyms(term,'NOUN')\n  for (i in 1:length(synos)){\n    str <- gsub(synos[i],term,str)\n  }\n  return(str)\n}\nstr <- syno_str_replace('war',str)\nnchar(text.v)\ntext.v <- syno_str_replace('joy',tolower(text.v))\nnchar(text.v)\n\n\n###  preprocessing with tm (& NLP) package(s)\nlibrary(tm)\nhelp(package = tm)\nls(\"package:tm\")\n\n# [back to] line 30\ntext.v <- paste(textlines.v, collapse = \" \")\n\n# convert to corpus\ntext.vs <- VectorSource(text.v)# create a corpus from character vectors\n\ntext.cor <- Corpus(text.vs)\nprint(text.cor)\n\n# preprocess\n#text.cor <- tm_map(text.cor, PlainTextDocument)\ntext.cor <- tm_map(text.cor, content_transformer(tolower))\ntext.cor <- tm_map(text.cor, removePunctuation)\ntext.cor <- tm_map(text.cor, removeNumbers)\ntext.cor <- tm_map(text.cor, removeWords, stopwords(\"english\"))\ntext.cor <- tm_map(text.cor, stemDocument)\ntext.cor <- tm_map(text.cor, stripWhitespace)\n\n# visualize\nlibrary(wordcloud)\ndev.new()\nwordcloud(text.cor,scale=c(3,1),max.words=50,random.order=FALSE,rot.per=0.40, use.r.layout=FALSE)\n\n# append corpus to save file\nresave(text.cor, file = 'kjv.RData')\n\n# scale to multiple documents with tm\n\nlibrary(tm)\n\n################################################################ this is for my supervisor says somebody tall\nsessionInfo()# check attached packages\ndd = \"/home/kln/Documents/education/tm_R/some_r/data/kjv_books\";\n# setwd(dd)\n\n# import plain text files in the directory dd containing Latin (lat) texts\n\nbooks.cor  <- Corpus(DirSource(dd, encoding = \"UTF-8\"), readerControl = list(language = \"lat\"))\nnames(books.cor) <- gsub(\"\\\\..*\",\"\",names(books.cor))# remove ending\nfilenames <- names(books.cor)\n# view documents\nbooks.cor[[2]]$content\nbooks.cor[[2]]$meta\n\n# preprocess\nbooks.cor <- tm_map(books.cor, PlainTextDocument)\nbooks.cor <- tm_map(books.cor, content_transformer(tolower))\nbooks.cor <- tm_map(books.cor, removePunctuation)\nbooks.cor <- tm_map(books.cor, removeNumbers)\nbooks.cor <- tm_map(books.cor, removeWords, stopwords(\"english\"))\nbooks.cor <- tm_map(books.cor, stemDocument)\nbooks.cor <- tm_map(books.cor, stripWhitespace)\n\n# remove sparse items\n\n### add metadata\nnames(books.cor) <- filenames\n# metadata at corpus level w. user-defined tags\nmeta(books.cor, type = 'corpus')\nmeta(books.cor, tag = 'collection', type = 'corpus') <- \"bible\"\nmeta(books.cor, tag = 'version', type = 'corpus') <- \"kjv\"\nmeta(books.cor, type = 'corpus')\n\n# metadata at document level\nmeta(books.cor, type = 'local')\nbooks.cor[[2]]$meta$author <- 'paul'\nbooks.cor[[2]]$meta$description <- 'letter'\nbooks.cor[[2]]$meta$language <- 'english'\nbooks.cor[[2]]$meta$origin <- 54\n\n# add external metadata at document level\nsetwd(wd)\ntmp <- read.csv('kjv_metadata.csv',header = TRUE)\nhead(tmp)\nfor (i in 1:length(books.cor)){\n  books.cor[[i]]$meta$heading <- as.character(tmp$filename[[i]])# pre-defined tag\n  books.cor[[i]]$meta$collection <- as.character(tmp$collection[[i]])# user-defined tags\n  books.cor[[i]]$meta$class <- as.character(tmp$class[[i]])\n}\nmeta(books.cor, type = 'local', tag = 'heading')\nmeta(books.cor, type = 'local', tag = 'collection')\n\n# predefined attributes\n# filter on metadata\nidx <- meta(books.cor, type = 'local',\"collection\") == 'new'\nnt.cor <- books.cor[idx]# new testament\n\n# append corpus to save file\nresave(books.cor, file = 'kjv.RData')",
    "created" : 1480232522612.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1491463397",
    "id" : "812C64C",
    "lastKnownWriteTime" : 1477489120,
    "last_content_update" : 1477489120,
    "path" : "~/Documents/education/tm_R/classes/tutorials/data_preparation.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}